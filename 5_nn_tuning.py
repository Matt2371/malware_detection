### TUNE PYTORCH NEURAL NETWORK MODEL USING TRAIN/VAL SPLIT AND EXHAUSTIVE SEARCH ###

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split

from src.models.nn_model import NN_model
from src.models.grid_search import exhaustive_grid
from src.models.train_pytorch_model import training_loop
from src.data_processing.feature_selection import rf_feature_selection

## CONDUCT GRID SEARCH ##

def grid_search(dataloader_train, dataloader_val, input_size):
    """Conduct grid search and save results as csv"""

    # Define hyperparameter space
    names = ['n_layers', 'hidden_size', 'dropout_prob', 'lr']
    arrays = [[1, 2, 3], [40, 50, 60], [0.3, 0.5, 0.7], [0.001, 0.0001]]
    grid = exhaustive_grid(arrays=arrays, names=names) # dataframe of shape (# runs, # params)
    results = grid.copy() # dataframe to save results
    results['epochs_trained'] = np.zeros(grid.shape[0])
    results['val_error'] = np.zeros(grid.shape[0])

    # Loop over grid
    for i in range(grid.shape[0]):
        # Select row of parameters
        params_i = grid.iloc[i, :]
        n_layers = int(params_i.n_layers)
        hidden_size = int(params_i.hidden_size)
        dropout_prob = params_i.dropout_prob
        lr = params_i.lr

        output_size = 1 # [output probability]

        # Instantiate model
        torch.manual_seed(0)
        model_tune = NN_model(input_size=input_size, hidden_size=hidden_size, n_layers=n_layers, lr=lr, 
                                dropout_prob=dropout_prob, output_size=output_size)
        criterion = nn.BCELoss()
        optimizer = optim.Adam(model_tune.parameters(), lr=model_tune.lr)

        # Run training loop and get validation error
        train_losses, val_losses = training_loop(model=model_tune, criterion=criterion, optimizer=optimizer,
                                                 patience=10, dataloader_train=dataloader_train, 
                                                 dataloader_val=dataloader_val, epochs=300)
        
        # Update results
        results['epochs_trained'].iloc[i] = len(val_losses)
        results['val_error'].iloc[i] = val_losses[-1]

    # Save results
    results.to_csv('report/results/hyperparameter_tuning/neural_network_tuning.csv')

## TRAIN AND SAVE MODEL WITH OPTIMAL HYPERPARAMETERS ##

def train_optimal_model(dataloader_train, dataloader_val, input_size):
    """Train and save optimal model based on results from grid search"""
    ## Train model with optimal hyperparameters and save
    # Find optimal hyperparameters
    # Load in results from grid search
    grid_df = pd.read_csv('report/results/hyperparameter_tuning/neural_network_tuning.csv', index_col=0)
    # Save sorted df
    grid_df.sort_values(by=['val_error'], axis=0, inplace=True)
    grid_df.to_csv('report/results/hyperparameter_tuning/neural_network_tuning.csv', index=False)

    # Instantiate optiamal model
    input_size = input_size
    output_size = 1
    hidden_size = int(grid_df.iloc[0].hidden_size)
    dropout_prob = grid_df.iloc[0].dropout_prob
    n_layers = int(grid_df.iloc[0].n_layers)
    lr = grid_df.iloc[0].lr

    torch.manual_seed(0)
    model = NN_model(input_size=input_size, hidden_size=hidden_size, n_layers=n_layers, lr=lr, 
                                dropout_prob=dropout_prob, output_size=output_size)
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=model.lr)

    # Run training loop
    train_losses, val_losses = training_loop(model=model, criterion=criterion, optimizer=optimizer,
                                                 patience=10, dataloader_train=dataloader_train, 
                                                 dataloader_val=dataloader_val, epochs=300)
    # Plot train/validation plot
    plt.clf()
    plt.plot(train_losses)
    plt.plot(val_losses)
    plt.legend(['training loss', 'validation loss'])
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.savefig('report/results/hyperparameter_tuning/neural_network_training.png')

    # Save model
    torch.save(model.state_dict(), 'report/results/saved_models/neural_network.pt')
    return

def main(input_size=40, batch_size=32):
    """
    input_size : # of features in input data (# of selected top features)
    batch_size : # of samples to take for minibatching
    """
    # read data, separate features (X) and targets (y)
    df = pd.read_csv('data/data_train.csv')

    # split data into train and validation sets
    df_train, df_val = train_test_split(df, test_size=0.2, shuffle=True, random_state=0)
    
    # separate features X and targets y
    X, y = df.drop(['Result'], axis=1), df.Result
    X_train, y_train = df_train.drop(['Result'], axis=1), df_train.Result
    X_val, y_val = df_val.drop(['Result'], axis=1), df_val.Result

    # calculate feature importance (on entire training set)
    rf_features = rf_feature_selection()
    rf_features.fit(X=X, y=y)
    sorted_features = rf_features.sorted_features

    # select top n features
    X_train = X_train.loc[:, sorted_features[:input_size]]
    X_val = X_val.loc[:, sorted_features[:input_size]]

    # convert everything to torch tensor
    X_train, X_val, y_train, y_val = (torch.tensor(X_train.values, dtype=torch.float32),
                                      torch.tensor(X_val.values, dtype=torch.float32),
                                      torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32),
                                      torch.tensor(y_val.values.reshape(-1, 1), dtype=torch.float32))

    # create PyTorch dataset and dataloader
    dataset_train, dataset_val = (TensorDataset(X_train, y_train), 
                                  TensorDataset(X_val, y_val))
    dataloader_train, dataloader_val = (DataLoader(dataset_train, batch_size=batch_size, shuffle=False), 
                                                        DataLoader(dataset_val, batch_size=batch_size, shuffle=False))
    
    # conduct grid search
    grid_search(dataloader_train=dataloader_train, dataloader_val=dataloader_val, input_size=input_size)

    # train and save optimal model
    train_optimal_model(dataloader_train=dataloader_train, dataloader_val=dataloader_val, input_size=input_size)

    return

# run script
if __name__ == '__main__':
    main()