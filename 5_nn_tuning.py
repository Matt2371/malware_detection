### TUNE PYTORCH NEURAL NETWORK MODEL USING TRAIN/VAL SPLIT AND EXHAUSTIVE SEARCH ###

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

from src.models.nn_model import NN_model
from src.models.grid_search import exhaustive_grid
from src.models.train_pytorch_model import training_loop
from src.data_processing.feature_selection import rf_feature_selection

## CONDUCT GRID SEARCH ##
## FIXME: NEED TO COMPELTE - ADD PERF CRITERIA AND COMPLETE GRID SEARCH

def grid_search(dataloader_train, dataloader_val):
    """Conduct grid search and save results as csv"""

    # Define hyperparameter space
    names = ['n_layers', 'hidden_size', 'dropout_prob', 'lr']
    arrays = [[1, 2, 3], [40, 50, 60], [0.3, 0.5, 0.7], [0.001, 0.0001]]
    grid = exhaustive_grid(arrays=arrays, names=names) # dataframe of shape (#runs, 5 (# params))
    results = grid.copy() # dataframe to save results
    results['epochs_trained'] = np.zeros(grid.shape[0])
    results['val_error'] = np.zeros(grid.shape[0])

    # Loop over grid
    for i in range(grid.shape[0]):
        # Select row of parameters
        params_i = grid.iloc[i, :]
        num_layers = int(params_i.num_layers)
        hidden_size1 = int(params_i.hidden1)
        hidden_size2 = int(params_i.hidden2)
        dropout_prob = params_i.dropout
        random_seed = params_i.random_seed

        input_size = 2 # [inflow, doy]
        output_size = 1 # [outflow]

        # Instantiate model
        torch.manual_seed(random_seed)
        model1c_tune = LSTMModel1_opt(input_size=input_size, hidden_size1=hidden_size1, 
                                hidden_size2=hidden_size2, output_size=output_size, num_layers=num_layers, dropout_prob=dropout_prob)
        criterion = RMSLELoss()
        optimizer = optim.Adam(model1c_tune.parameters(), lr=0.001)

        # Run training loop and get validation error
        train_losses, val_losses = training_loop(model=model1c_tune, criterion=criterion, optimizer=optimizer, 
                                                patience=10, dataloader_train=dataloader_train, dataloader_val=dataloader_val, epochs=300)
        
        # Update results
        results['epochs_trained'].iloc[i] = len(val_losses)
        results['val_error'].iloc[i] = val_losses[-1]

    # Save results
    results.to_csv('report/results/hyperparameter_tuning/model1c_tuning.csv')

## TRAIN AND SAVE MODEL WITH OPTIMAL HYPERPARAMETERS ##

def train_optimal_model_1c(dataloader_train, dataloader_val):
    """Train and save optimal model based on results from grid search"""
    ## Train model with optimal hyperparameters and save
    # Find optimal hyperparameters
    # Load in results from grid search
    grid_df = pd.read_csv('report/results/hyperparameter_tuning/model1c_tuning.csv', index_col=0)
    # Average performance over the random seeds
    num_random_seeds = 5
    grid_df['param_id'] = np.repeat(np.arange(int(len(grid_df) / num_random_seeds)), num_random_seeds)
    grid_df_mean = grid_df.groupby('param_id').mean()
    grid_df_mean.drop(columns=['random_seed'], inplace=True)
    # Save sorted df
    grid_df_mean.sort_values(by=['val_error'], axis=0, inplace=True)
    grid_df_mean.to_csv('report/results/hyperparameter_tuning/model1c_avg_tuning.csv')

    # Instantiate optiamal model
    input_size = 2
    hidden_size1 = int(grid_df_mean.iloc[0].hidden1)
    hidden_size2 = int(grid_df_mean.iloc[0].hidden2)
    output_size = 1
    dropout_prob = grid_df_mean.iloc[0].dropout
    num_layers = int(grid_df_mean.iloc[0].num_layers)

    torch.manual_seed(0)
    model1c = LSTMModel1(input_size=input_size, hidden_size1=hidden_size1, 
                                hidden_size2=hidden_size2, output_size=output_size, num_layers=num_layers, dropout_prob=dropout_prob)
    criterion = RMSLELoss()
    optimizer = optim.Adam(model1c.parameters(), lr=0.001)

    # Run training loop
    train_losses, val_losses = training_loop(model=model1c, criterion=criterion, optimizer=optimizer, 
                                            patience=10, dataloader_train=dataloader_train, 
                                            dataloader_val=dataloader_val, epochs=300)
    # Plot train/validation plot
    plot_train_val(train_losses=train_losses, val_losses=val_losses)
    plt.savefig('jobs/model1c_training.png')

    # Save model
    torch.save(model1c.state_dict(), 'src/models/saved_models/model1c.pt')
    return

# run script
if __name__ == '__main__':
    # data processing
    dataloader_train, dataloader_val = data_processing_1c(name='Shasta', transform_type='normalize')

    # # grid search
    # grid_search_1c(dataloader_train=dataloader_train, dataloader_val=dataloader_val)

    # save optimal model
    train_optimal_model_1c(dataloader_train=dataloader_train, dataloader_val=dataloader_val)