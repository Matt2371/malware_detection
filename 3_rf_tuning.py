### TUNE RANDOM FOREST MODEL USING K-FOLD CV AND EXHAUSTIVE SEARCH ###


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier
from joblib import dump, load

from src.models.grid_search import grid_search
from src.models.grid_search import get_best_params
from src.models.k_fold_cv import RF_cv
from src.data_processing.feature_selection import rf_feature_selection

def avg_rf_cv_func(k_folds, X, y, n_estimators, max_features):
    """
    Corresponds to avg_cv_func argument in models.grid_search.grid_search() for random forest
    Evaluate random forest under k-fold cross validation on the training data
    and return average performance over folds.
    """

    # Get k-fold indices
    kf = KFold(n_splits=k_folds, random_state=1, shuffle=True)

    # Get average results over k folds
    # instantiate cross validation object for random forest
    cross_val = RF_cv(X=X, y=y, kf=kf, n_estimators=n_estimators, max_features=max_features)
    cross_val.run_cv()
    avg_result = cross_val.average_perf()
    return avg_result

def main(n_top_features, filepath):
    """ 
    1. Read training data / select top n rf features
    2. Conduct grid search
    3. Retrain and save model based on best parameters from grid search
    Params:
    n_top_features: int, number of top features to select
    filepath: str, filepath to save final model
    """
    # Read training data
    df = pd.read_csv('data/data_train.csv')
    X, y = df.drop(['Result'], axis=1), df.Result

    # Calculate feature importance
    rf_features = rf_feature_selection()
    rf_features.fit(X=X, y=y)
    sorted_features = rf_features.sorted_features

    # Select n top features
    X = X.loc[:, sorted_features[:n_top_features]]

    # Define search space
    n_estimators_array = np.arange(start=100, stop=600, step=100)
    max_features_array = [round(i * n_top_features) for i in np.arange(start=0.1, stop=1, step=0.1)]
    arrays = [n_estimators_array, max_features_array]
    names = ['n_estimators', 'max_features']

    # Conduct k-fold CV over grid search
    result = grid_search(X=X, y=y, arrays=arrays, names=names, avg_cv_func=avg_rf_cv_func)

    # Save results as csv
    result.to_csv('report/results/hyperparameter_tuning/random_forest_grid_search.csv', index=False)

    # Retrain model with best params
    best_params = get_best_params(grid_search_results=result, perf_measure='f1_score').astype(np.int32)
    final_model = RandomForestClassifier(**best_params)
    final_model.fit(X=X, y=y)

    # Save final model
    dump(final_model, filepath)

    return

# run script
if __name__ == '__main__':
    main(n_top_features=40, filepath='report/results/saved_models/random_forest.joblib')
