### TUNE LOGISTIC REGRESSION MODEL USING K-FOLD CV AND EXHAUSTIVE SEARCH ###


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold

from src.models.grid_search import exhaustive_grid
from src.models.k_fold_cv import LG_cv
from src.data_processing.feature_selection import rf_feature_selection

def eval_logistic_cv(k_folds, X, y, C=1, penalty='l2'):
    """
    Evaluate logistic regression under k-fold cross validation on the training data
    and return average performance over folds
    """

    # Get k-fold indices
    num_folds = 10
    kf = KFold(n_splits=num_folds, random_state=1, shuffle=True)

    # Get average results over k folds
    cross_val = LG_cv(X=X, y=y, kf=kf, C=C, penalty=penalty) # instantiate cross validation object
    cross_val.run_cv()
    avg_result = cross_val.average_perf()
    return avg_result

def grid_search(X, y, arrays, names):
    """
    Conduct grid search over defined parameters and save results as pandas df
    Params:
    arrays -- list-like of arrays defining search space
    names -- names (of hyperparameters) corresponding to arrays
    (X, y) -- training features and targets
    Returns:
    dataframe with results for all pairs of parameters
    """
    # Create grid to search over
    grid = exhaustive_grid(arrays=arrays, names=names)
    results = grid.copy()

    # Add column to save results
    results['accuracy'] = np.zeros(grid.shape[0])
    results['f1_score'] = np.zeros(grid.shape[0])
    results['precision'] = np.zeros(grid.shape[0])
    results['recall'] = np.zeros(grid.shape[0])
    results['auc_roc'] = np.zeros(grid.shape[0])

    # Loop over grid
    for i in range(grid.shape[0]):
        # Select i_th pairs of hyperparameters
        params_i = grid.iloc[i, :]
        C_i = params_i.C 
        penalty_i = params_i.penalty

        # Run k-fold CV and return average results
        avg_result_i = eval_logistic_cv(k_folds=10, X=X, y=y, C=C_i, penalty=penalty_i)
        results.iloc[i, :]['accuracy'] = avg_result_i.accuracy
        results.iloc[i, :]['f1_score'] = avg_result_i.f1_score
        results.iloc[i, :]['precision'] = avg_result_i.precision
        results.iloc[i, :]['recall'] = avg_result_i.recall
        results.iloc[i, :]['auc_roc'] = avg_result_i.auc_roc
    return results

def main(n_top_features):
    # Read training data
    df = pd.read_csv('data/data_train.csv')
    X, y = df.drop(['Result'], axis=1), df.Result

    # Calculate feature importance
    rf_features = rf_feature_selection()
    rf_features.fit(X=X, y=y)
    sorted_features = rf_features.sorted_features

    # Select n top features
    X = X.loc[:, sorted_features[:n_top_features]]

    # Conduct k-fold CV over grid search
    result = grid_search(X=X, y=y)

    # Save results as csv
    result.to_csv('report/results/hyperparameter_tuning/logistic_regression_grid_search.csv')
    return

# run script
if __name__ == '__main__':
    main(n_top_features=40)
