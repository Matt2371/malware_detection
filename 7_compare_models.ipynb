{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare performance of trained classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from joblib import load\n",
    "\n",
    "from src.models.evaluate import evaluate\n",
    "from src.models.nn_model import NN_model\n",
    "from src.models.ensemble_stacking import StackingEnsemble\n",
    "from src.data_processing.feature_selection import rf_feature_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance metrics\n",
    "Compare accuracy, f1 score, precision, recall, roc auc score on the hold-out test data for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "df_train = pd.read_csv('data/data_train.csv')\n",
    "df_test = pd.read_csv('data/data_test.csv')\n",
    "\n",
    "# separate features X and targets y\n",
    "X_train, y_train = df_train.drop(['Result'], axis=1), df_train.Result\n",
    "X_test, y_test = df_test.drop(['Result'], axis=1), df_test.Result\n",
    "\n",
    "# calculate feature importance\n",
    "input_size = 40 # number of top features to select\n",
    "rf_features = rf_feature_selection()\n",
    "rf_features.fit(X=X_train, y=y_train)\n",
    "sorted_features = rf_features.sorted_features\n",
    "\n",
    "# select top n features\n",
    "X_test = X_test.loc[:, sorted_features[:input_size]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load logistic regression\n",
    "model_lg = load('report/results/saved_models/logistic_regression.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.959945457644452,\n",
       " 'f1': 0.9601762413150314,\n",
       " 'precision': 0.9558029689608637,\n",
       " 'recall': 0.9645897173987061,\n",
       " 'roc_auc': 0.9882392587918531}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get test performance\n",
    "y_hat_lg = model_lg.predict_proba(X_test)[:, 1]\n",
    "pred_labels_lg = model_lg.predict(X_test)\n",
    "perf_lg = evaluate(y_true=y_test, pred_scores=y_hat_lg, pred_labels=pred_labels_lg)\n",
    "perf_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load random forest\n",
    "model_rf = load('report/results/saved_models/random_forest.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9691494801431737,\n",
       " 'f1': 0.9690651170740043,\n",
       " 'precision': 0.9728894989704873,\n",
       " 'recall': 0.965270684371808,\n",
       " 'roc_auc': 0.9948611396784115}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get test performance\n",
    "y_hat_rf = model_rf.predict_proba(X_test)[:, 1]\n",
    "pred_labels_rf = model_rf.predict(X_test)\n",
    "perf_rf = evaluate(y_true=y_test, pred_scores=y_hat_rf, pred_labels=pred_labels_rf)\n",
    "perf_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load knn model\n",
    "model_knn = load('report/results/saved_models/knn.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9623316856996762,\n",
       " 'f1': 0.9627507163323782,\n",
       " 'precision': 0.9532710280373832,\n",
       " 'recall': 0.9724208375893769,\n",
       " 'roc_auc': 0.9856190466229964}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get test performance\n",
    "y_hat_knn = model_knn.predict_proba(X_test)[:, 1]\n",
    "pred_labels_knn = model_knn.predict(X_test)\n",
    "perf_knn = evaluate(y_true=y_test, pred_scores=y_hat_knn, pred_labels=pred_labels_knn)\n",
    "perf_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN_model(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=40, out_features=60, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=60, out_features=60, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=60, out_features=60, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.3, inplace=False)\n",
       "    (9): Linear(in_features=60, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load grid search results\n",
    "grid_df = pd.read_csv('report/results/hyperparameter_tuning/neural_network_tuning.csv')\n",
    "\n",
    "# Instantiate optiamal model\n",
    "output_size = 1\n",
    "hidden_size = int(grid_df.iloc[0].hidden_size)\n",
    "dropout_prob = grid_df.iloc[0].dropout_prob\n",
    "n_layers = int(grid_df.iloc[0].n_layers)\n",
    "lr = grid_df.iloc[0].lr\n",
    "\n",
    "model_nn = NN_model(input_size=input_size, hidden_size=hidden_size, n_layers=n_layers, lr=lr, \n",
    "                            dropout_prob=dropout_prob, output_size=output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_nn.parameters(), lr=model_nn.lr)\n",
    "\n",
    "# Load saved model\n",
    "model_nn.load_state_dict(torch.load('report/results/saved_models/neural_network.pt'))\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model_nn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the test set\n",
    "y_hat_nn = model_nn(torch.from_numpy(X_test.values).to(torch.float32)).detach().numpy().flatten()\n",
    "# Convert predicted probabilities to predicted labels\n",
    "pred_labels_nn = (y_hat_nn > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.969660814726436,\n",
       " 'f1': 0.9694787379972565,\n",
       " 'precision': 0.9765112262521589,\n",
       " 'recall': 0.9625468164794008,\n",
       " 'roc_auc': 0.992915445051427}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get performance metrics\n",
    "nn_perf = evaluate(y_true=y_test, pred_scores=y_hat_nn, pred_labels=pred_labels_nn)\n",
    "nn_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stacking model\n",
    "model_st = load('report/results/saved_models/stacking_ensemble.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9684677006988239,\n",
       " 'f1': 0.9684246458440007,\n",
       " 'precision': 0.9709103353867214,\n",
       " 'recall': 0.9659516513449098,\n",
       " 'roc_auc': 0.9951804736787673}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get test performance\n",
    "y_hat_st = model_st.predict_proba(X_test)[:, 1]\n",
    "pred_labels_st = model_st.predict(X_test)\n",
    "perf_st = evaluate(y_true=y_test, pred_scores=y_hat_st, pred_labels=pred_labels_st)\n",
    "perf_st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_lg, perf_rf, perf_knn, nn_perf, perf_st = (pd.Series(perf_lg, name='LG'), pd.Series(perf_rf, name='RF'), \n",
    "                                                pd.Series(perf_knn, name='kNN'), pd.Series(nn_perf, name='FF'), \n",
    "                                                pd.Series(perf_st, name='ST'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LG</th>\n",
       "      <th>RF</th>\n",
       "      <th>kNN</th>\n",
       "      <th>FF</th>\n",
       "      <th>ST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.960</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.960</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.956</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.965</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc</th>\n",
       "      <td>0.988</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              LG     RF    kNN     FF     ST\n",
       "accuracy   0.960  0.969  0.962  0.970  0.968\n",
       "f1         0.960  0.969  0.963  0.969  0.968\n",
       "precision  0.956  0.973  0.953  0.977  0.971\n",
       "recall     0.965  0.965  0.972  0.963  0.966\n",
       "roc_auc    0.988  0.995  0.986  0.993  0.995"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.concat([perf_lg, perf_rf, perf_knn, nn_perf, perf_st], axis=1)\n",
    "results_df.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance is nearly identical between the models, though random forest, the feed forward neural network, and the stacking ensemble perform slightly better than logistic regression or kNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Time\n",
    "Compare evaluation time between the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_time(X, model):\n",
    "    \"\"\"Return time to evaluate data sample\"\"\"\n",
    "    if isinstance(model, nn.Module):\n",
    "        model.eval()\n",
    "        X_ts = torch.from_numpy(X_test.values).to(torch.float32)\n",
    "        begin = time.time()\n",
    "        eval = model(X_ts)\n",
    "        length = time.time() - begin\n",
    "    else:\n",
    "        begin = time.time()\n",
    "        eval = model.predict_proba(X)\n",
    "        length = time.time() - begin\n",
    "        \n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make up data\n",
    "data = pd.Series(data=np.random.binomial(size=X_test.shape[1], n=1, p=0.5), index=X_test.columns).to_frame().T\n",
    "\n",
    "# get eval time\n",
    "eval_times = pd.Series([eval_time(data, model_lg), eval_time(data, model_rf),\n",
    "                       eval_time(data, model_knn), eval_time(data, model_nn),\n",
    "                       eval_time(data, model_st)], index=['LG', 'RF', 'kNN', 'FF', 'ST'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LG     0.000000\n",
       "RF     0.039495\n",
       "kNN    0.007993\n",
       "FF     0.005944\n",
       "ST     0.196457\n",
       "dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
