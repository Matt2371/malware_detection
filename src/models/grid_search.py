import numpy as np
import pandas as pd

### FUNCTIONS TO ASSIST WITH HYPERPARAMETER TUNING ###

def exhaustive_grid(arrays, names):
    """ 
    Returns a 2d dataframe grid with shape (# of runs, # of parameters to be tuned). Contains
    (exhaustively) every pair of hyperparameters to try.
    Params:
    arrays: tuple/list of arrays defining the hyperparameter space
    names: list of names corresponding to hyperparameters in arrays 
    (MUST coincide with instatiation parameters for desired model object)
    """

    assert len(arrays) == len(names)

    # Get meshgrids
    meshgrid = np.meshgrid(*arrays)

    # Flatten and create 2d array (# runs, # hyperparameters)
    result = np.vstack([grid.reshape(-1) for grid in meshgrid]).T
    
    # Convert to pandas dataframe
    result = pd.DataFrame(result, columns=names)

    return result

def grid_search(X, y, arrays, names, avg_cv_func, k_folds=10):
    """
    Conduct grid search over defined parameters and save performance results as pandas df
    Params:
    (X, y) -- training features and targets
    arrays -- list-like of arrays defining search space, corresponding to exhaustive_grid()
    names -- names (of hyperparameters) corresponding to arrays, corresponding to exhaustive_grid()
    avg_cv_func(k_folds, X, y, **params) -- function that returns average performance measures over k folds on desired model

    Returns:
    results -- pandas dataframe with results for all pairs of parameters
    """
    # Create grid to search over
    grid = exhaustive_grid(arrays=arrays, names=names)
    results = grid.copy()

    # Add column to save results
    results['accuracy'] = np.zeros(grid.shape[0])
    results['f1_score'] = np.zeros(grid.shape[0])
    results['precision'] = np.zeros(grid.shape[0])
    results['recall'] = np.zeros(grid.shape[0])
    results['roc_auc'] = np.zeros(grid.shape[0])

    # Loop over grid
    for i in range(grid.shape[0]):
        # Select i_th pairs of hyperparameters
        params_i = grid.iloc[i, :].to_dict() # corresponds to **params arguments in model

        # Run k-fold CV and return average results
        avg_result_i = avg_cv_func(k_folds=k_folds, X=X, y=y, **params_i)
        results.loc[results.index[i], 'accuracy'] = avg_result_i.accuracy
        results.loc[results.index[i], 'f1_score'] = avg_result_i.f1_score
        results.loc[results.index[i], 'precision'] = avg_result_i.precision
        results.loc[results.index[i], 'recall'] = avg_result_i.recall
        results.loc[results.index[i], 'roc_auc'] = avg_result_i.roc_auc

    return results

def get_best_params(grid_search_results, perf_measure='f1_score'):
    """ 
    Get pandas series of best combination of parameters
    Params:
    grid_search_results - pandas df of grid search results (as outputted by grid_search())
    perf_measure - str, desired performance measure to select best params, 
                ['accuracy', 'f1_score', 'precision', 'recall', 'roc_auc']
    """
    # Find best params based on perf_measure
    best_params = grid_search_results.iloc[grid_search_results[perf_measure].argmax(), :]
    # Drop performance measures from Series
    best_params = best_params.drop(['accuracy', 'f1_score', 'precision', 'recall', 'roc_auc'])

    return best_params

    


