### CODE TO CONDUCT K-FOLD CROSS VALIDATION FOR EACH MODEL ###

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import roc_auc_score


def evaluate(y_true, pred_scores, pred_labels):
    """
    Return dict of model metrics, including accuracy, precision, recall, f1, and roc auc
    y_true -- true target labels
    pred_scores -- predicted probability (for positive label)
    pred_labels -- predicted labels
    """
    accuracy = accuracy_score(y_true=y_true, y_pred=pred_labels)
    f1 = f1_score(y_true=y_true, y_pred=pred_labels)
    precision = precision_score(y_true=y_true, y_pred=pred_labels)
    recall = recall_score(y_true=y_true, y_pred=pred_labels)
    roc_auc = roc_auc_score(y_true=y_true, y_score=pred_scores)

    return {'accuracy': accuracy,
            'f1': f1,
            'precision': precision,
            'recall': recall,
            'roc_auc': roc_auc}


class LG_cv:
    """
    Perform k-fold cross validation for logistic regression model (with L2 penality) and
    calculates performance measures (accuracy, f1, precision, recall, roc auc) for each fold
    """
    def __init__(self, X, y, kf, C=1, penalty='l2'):
        """
        https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
        X - features, pandas dataframe of shape (n, p)
        y - targets, pandas dataframe of shape (n)
        kf - sklearn.model_selection.KFold object
        C - inverse of regularization strength
        """
        self.kf = kf
        self.X = X
        self.y = y

        # Hyperparameters
        self.C = C
        self.penalty = penalty

        # Dataframe to save performance metrics for each fold
        self.result_df = pd.DataFrame()
        self.isfit = False
        return
    
    def run_cv(self):
        """Run k-fold cross validation and save performance for each fold"""
        # Initialize lists to store results for each fold
        accuracies = []
        f1 = []
        precision = []
        recall = []
        roc_auc = []

        # Loop over the k-fold cross validation indices
        for train_index, test_index in self.kf.split(self.X):
            # Access the data
            X_train, X_test = self.X.iloc[train_index, :], self.X.iloc[test_index, :]
            y_train, y_test = self.y[train_index], self.y[test_index]

            # Instantiate model
            model = LogisticRegression(C=self.C, penalty=self.penalty)

            # Train on current fold (training data)
            model.fit(X_train, y_train)

            # Make predictions on current fold (test data)
            pred_labels = model.predict(X=X_test)
            pred_scores = model.predict_proba(X=X_test)[:, 1]

            # Evaluate on current fold
            results_dict = evaluate(y_true=y_test, pred_labels=pred_labels, pred_scores=pred_scores)
            accuracies.append(results_dict['accuracy'])
            f1.append(results_dict['f1'])
            precision.append(results_dict['precision'])
            recall.append(results_dict['recall'])
            roc_auc.append(results_dict['roc_auc'])
        
        # Save dataframe of performance metrics for each fold
        self.result_df = pd.DataFrame(data={'accuracy': accuracies,
                               'f1_score': f1,
                               'precision': precision,
                               'recall': recall,
                               'roc_auc': roc_auc})
        self.isfit = True
    
    def average_perf(self):
        """Returns pandas series of average performance over all folds for each evaluation metric"""
        if self.isfit == False:
            raise Exception("CV has not been run")
        else:
            return self.result_df.mean(axis=0)
        

class kNN_cv:
    """
    Perform k-fold cross validation for k-nearest neighbors model and
    calculates performance measures (accuracy, f1, precision, recall, roc auc) for each fold
    """
    def __init__(self, X, y, kf, n_neighbors=5, p=2):
        """
        https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html
        X - features, pandas dataframe of shape (n, p)
        y - targets, pandas dataframe of shape (n)
        kf - sklearn.model_selection.KFold object
        n_neighbors - # of neighbors
        p - type of distance (i.e. p=1 is L1 norm and p=2 is L2 norm)
        """
        self.kf = kf
        self.X = X
        self.y = y

        # Hyperparameters
        self.n_neighbors = n_neighbors
        self.p = p

        # Dataframe to save performance metrics for each fold
        self.result_df = pd.DataFrame()
        self.isfit = False
        return
    
    def run_cv(self):
        """Run k-fold cross validation and save performance for each fold"""
        # Initialize lists to store results for each fold
        accuracies = []
        f1 = []
        precision = []
        recall = []
        roc_auc = []

        # Loop over the k-fold cross validation indices
        for train_index, test_index in self.kf.split(self.X):
            # Access the data
            X_train, X_test = self.X.iloc[train_index, :], self.X.iloc[test_index, :]
            y_train, y_test = self.y[train_index], self.y[test_index]

            # Instantiate model
            model = KNeighborsClassifier(n_neighbors=self.n_neighbors, p=self.p)

            # Train on current fold (training data)
            model.fit(X_train, y_train)

            # Make predictions on current fold (test data)
            pred_labels = model.predict(X=X_test)
            pred_scores = model.predict_proba(X=X_test)[:, 1]

            # Evaluate on current fold
            results_dict = evaluate(y_true=y_test, pred_labels=pred_labels, pred_scores=pred_scores)
            accuracies.append(results_dict['accuracy'])
            f1.append(results_dict['f1'])
            precision.append(results_dict['precision'])
            recall.append(results_dict['recall'])
            roc_auc.append(results_dict['roc_auc'])
        
        # Save dataframe of performance metrics for each fold
        self.result_df = pd.DataFrame(data={'accuracy': accuracies,
                               'f1_score': f1,
                               'precision': precision,
                               'recall': recall,
                               'roc_auc': roc_auc})
        self.isfit = True
    
    def average_perf(self):
        """Returns pandas series of average performance over all folds for each evaluation metric"""
        if self.isfit == False:
            raise Exception("CV has not been run")
        else:
            return self.result_df.mean(axis=0)
        
class RF_cv:
    """
    Perform k-fold cross validation for k-nearest neighbors model and
    calculates performance measures (accuracy, f1, precision, recall, roc auc) for each fold
    """
    def __init__(self, X, y, kf, n_estimators=100, max_features='sqrt'):
        """
        https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
        X - features, pandas dataframe of shape (n, p)
        y - targets, pandas dataframe of shape (n)
        kf - sklearn.model_selection.KFold object
        n_estimators - # of trees in forest
        """
        self.kf = kf
        self.X = X
        self.y = y

        # Hyperparameters
        self.n_estimators = n_estimators
        self.max_features = max_features

        # Dataframe to save performance metrics for each fold
        self.result_df = pd.DataFrame()
        self.isfit = False
        return
    
    def run_cv(self):
        """Run k-fold cross validation and save performance for each fold"""
        # Initialize lists to store results for each fold
        accuracies = []
        f1 = []
        precision = []
        recall = []
        roc_auc = []

        # Loop over the k-fold cross validation indices
        for train_index, test_index in self.kf.split(self.X):
            # Access the data
            X_train, X_test = self.X.iloc[train_index, :], self.X.iloc[test_index, :]
            y_train, y_test = self.y[train_index], self.y[test_index]

            # Instantiate model
            model = RandomForestClassifier(n_estimators=self.n_estimators, max_features=self.max_features)

            # Train on current fold (training data)
            model.fit(X_train, y_train)

            # Make predictions on current fold (test data)
            pred_labels = model.predict(X=X_test)
            pred_scores = model.predict_proba(X=X_test)[:, 1]

            # Evaluate on current fold
            results_dict = evaluate(y_true=y_test, pred_labels=pred_labels, pred_scores=pred_scores)
            accuracies.append(results_dict['accuracy'])
            f1.append(results_dict['f1'])
            precision.append(results_dict['precision'])
            recall.append(results_dict['recall'])
            roc_auc.append(results_dict['roc_auc'])
        
        # Save dataframe of performance metrics for each fold
        self.result_df = pd.DataFrame(data={'accuracy': accuracies,
                               'f1_score': f1,
                               'precision': precision,
                               'recall': recall,
                               'roc_auc': roc_auc})
        self.isfit = True
    
    def average_perf(self):
        """Returns pandas series of average performance over all folds for each evaluation metric"""
        if self.isfit == False:
            raise Exception("CV has not been run")
        else:
            return self.result_df.mean(axis=0)
        