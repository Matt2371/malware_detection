### CODE TO CONDUCT K-FOLD CROSS VALIDATION FOR EACH MODEL ###

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

from src.models.evaluate import evaluate

class LG_cv:
    """
    Perform k-fold cross validation for logistic regression model (with L2 penality) and
    calculates performance measures (accuracy, f1, precision, recall, roc auc) for each fold
    """
    def __init__(self, X, y, kf, C=1):
        """
        https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
        X - features, pandas dataframe of shape (n, p)
        y - targets, pandas dataframe of shape (n)
        kf - sklearn.model_selection.KFold object
        C - inverse of regularization strength
        """
        self.kf = kf
        self.X = X
        self.y = y

        # Hyperparameters
        self.C = C

        # Dataframe to save performance metrics for each fold
        self.result_df = pd.DataFrame()
        self.isfit = False
        return
    
    def run_cv(self):
        """Run k-fold cross validation and save performance for each fold"""
        # Initialize lists to store results for each fold
        accuracies = []
        f1 = []
        precision = []
        recall = []
        roc_auc = []

        # Loop over the k-fold cross validation indices
        for train_index, test_index in self.kf.split(self.X):
            # Access the data
            X_train, X_test = self.X.iloc[train_index, :], self.X.iloc[test_index, :]
            y_train, y_test = self.y[train_index], self.y[test_index]

            # Instantiate model
            model = LogisticRegression(C=self.C)

            # Train on current fold (training data)
            model.fit(X_train, y_train)

            # Make predictions on current fold (test data)
            pred_labels = model.predict(X=X_test)
            pred_scores = model.predict_proba(X=X_test)[:, 1]

            # Evaluate on current fold
            results_dict = evaluate(y_true=y_test, pred_labels=pred_labels, pred_scores=pred_scores)
            accuracies.append(results_dict['accuracy'])
            f1.append(results_dict['f1'])
            precision.append(results_dict['precision'])
            recall.append(results_dict['recall'])
            roc_auc.append(results_dict['roc_auc'])
        
        # Save dataframe of performance metrics for each fold
        self.result_df = pd.DataFrame(data={'accuracy': accuracies,
                               'f1 score': f1,
                               'precision': precision,
                               'recall': recall,
                               'roc auc': roc_auc})
        self.isfit = True
    
    def average_perf(self):
        """Returns pandas series of average performance over all folds for each evaluation metric"""
        if self.isfit == False:
            raise Exception("CV has not been run")
        else:
            return self.result_df.mean(axis=0)
        

class kNN_cv:
    """
    Perform k-fold cross validation for k-nearest neighbors model and
    calculates performance measures (accuracy, f1, precision, recall, roc auc) for each fold
    """
    def __init__(self, X, y, kf, n=5, p=2):
        """
        https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html
        X - features, pandas dataframe of shape (n, p)
        y - targets, pandas dataframe of shape (n)
        kf - sklearn.model_selection.KFold object
        n - # of neighbors
        p - type of distance (i.e. p=1 is L1 norm and p=2 is L2 norm)
        """
        self.kf = kf
        self.X = X
        self.y = y

        # Hyperparameters
        self.n = n
        self.p = p

        # Dataframe to save performance metrics for each fold
        self.result_df = pd.DataFrame()
        self.isfit = False
        return
    
    def run_cv(self):
        """Run k-fold cross validation and save performance for each fold"""
        # Initialize lists to store results for each fold
        accuracies = []
        f1 = []
        precision = []
        recall = []
        roc_auc = []

        # Loop over the k-fold cross validation indices
        for train_index, test_index in self.kf.split(self.X):
            # Access the data
            X_train, X_test = self.X.iloc[train_index, :], self.X.iloc[test_index, :]
            y_train, y_test = self.y[train_index], self.y[test_index]

            # Instantiate model
            model = KNeighborsClassifier(n_neighbors=self.n, p=self.p)

            # Train on current fold (training data)
            model.fit(X_train, y_train)

            # Make predictions on current fold (test data)
            pred_labels = model.predict(X=X_test)
            pred_scores = model.predict_proba(X=X_test)[:, 1]

            # Evaluate on current fold
            results_dict = evaluate(y_true=y_test, pred_labels=pred_labels, pred_scores=pred_scores)
            accuracies.append(results_dict['accuracy'])
            f1.append(results_dict['f1'])
            precision.append(results_dict['precision'])
            recall.append(results_dict['recall'])
            roc_auc.append(results_dict['roc_auc'])
        
        # Save dataframe of performance metrics for each fold
        self.result_df = pd.DataFrame(data={'accuracy': accuracies,
                               'f1 score': f1,
                               'precision': precision,
                               'recall': recall,
                               'roc auc': roc_auc})
        self.isfit = True
    
    def average_perf(self):
        """Returns pandas series of average performance over all folds for each evaluation metric"""
        if self.isfit == False:
            raise Exception("CV has not been run")
        else:
            return self.result_df.mean(axis=0)
        
class RF_cv:
    """
    Perform k-fold cross validation for k-nearest neighbors model and
    calculates performance measures (accuracy, f1, precision, recall, roc auc) for each fold
    """
    def __init__(self, X, y, kf, n_estimators=100):
        """
        https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
        X - features, pandas dataframe of shape (n, p)
        y - targets, pandas dataframe of shape (n)
        kf - sklearn.model_selection.KFold object
        n_estimators - # of trees in forest
        """
        self.kf = kf
        self.X = X
        self.y = y

        # Hyperparameters
        self.n_estimators = n_estimators

        # Dataframe to save performance metrics for each fold
        self.result_df = pd.DataFrame()
        self.isfit = False
        return
    
    def run_cv(self):
        """Run k-fold cross validation and save performance for each fold"""
        # Initialize lists to store results for each fold
        accuracies = []
        f1 = []
        precision = []
        recall = []
        roc_auc = []

        # Loop over the k-fold cross validation indices
        for train_index, test_index in self.kf.split(self.X):
            # Access the data
            X_train, X_test = self.X.iloc[train_index, :], self.X.iloc[test_index, :]
            y_train, y_test = self.y[train_index], self.y[test_index]

            # Instantiate model
            model = RandomForestClassifier(n_estimators=self.n_estimators)

            # Train on current fold (training data)
            model.fit(X_train, y_train)

            # Make predictions on current fold (test data)
            pred_labels = model.predict(X=X_test)
            pred_scores = model.predict_proba(X=X_test)[:, 1]

            # Evaluate on current fold
            results_dict = evaluate(y_true=y_test, pred_labels=pred_labels, pred_scores=pred_scores)
            accuracies.append(results_dict['accuracy'])
            f1.append(results_dict['f1'])
            precision.append(results_dict['precision'])
            recall.append(results_dict['recall'])
            roc_auc.append(results_dict['roc_auc'])
        
        # Save dataframe of performance metrics for each fold
        self.result_df = pd.DataFrame(data={'accuracy': accuracies,
                               'f1 score': f1,
                               'precision': precision,
                               'recall': recall,
                               'roc auc': roc_auc})
        self.isfit = True
    
    def average_perf(self):
        """Returns pandas series of average performance over all folds for each evaluation metric"""
        if self.isfit == False:
            raise Exception("CV has not been run")
        else:
            return self.result_df.mean(axis=0)
        